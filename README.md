# TokLIP: Marry Visual Tokens to CLIP for Multimodal Comprehension and Generation


Welcome to the official code repository for "[**TokLIP: Marry Visual Tokens to CLIP for Multimodal Comprehension and Generation**](https://arxiv.org/abs/2505.05422)".



### Code Availability
The code will be made available in a short time. Please check back soon or watch this repository to get notified upon release.


### Contact
For immediate queries or further information, please open an issue or contact <haokun.lin@cripac.ia.ac.cn>.


### Acknowledgement
This repo is build upon the following projects:

* [OpenCLIP](https://github.com/mlfoundations/open_clip)
* [LlamaGen](https://github.com/FoundationVision/LlamaGen)

We thank the authors for their code.

### Citation
Please cite our work if you use our code or discuss our findings in your own research:
```
@article{lin2025toklip,
  title={TokLIP: Marry Visual Tokens to CLIP for Multimodal Comprehension and Generation}
  author={Lin, Haokun and Wang, Teng and Ge, Yixiao and Ge, Yuying and Lu, Zhichao and Wei, Ying and Zhang, Qingfu and Sun, Zhenan and Shan, Ying},
  journal={arXiv preprint arXiv:2505.05422},
  year={2025}
}
```
